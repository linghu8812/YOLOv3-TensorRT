[net]
# Testing
# batch=1
# subdivisions=1
# Training
batch=64
subdivisions=16
width=416
height=416
channels=3
momentum=0.9
decay=0.0005
angle=0
saturation = 1.5
exposure = 1.5
hue=.1

learning_rate=0.001
burn_in=1000
max_batches = 100200
policy=steps
steps=80000,90000
scales=.1,.1


# conv1
[convolutional]
filters=32
size=3
pad=1
stride=2
batch_normalize=1
activation=relu

# conv2_1_expand
[convolutional]
filters=32
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv2_1_dwise
[convolutional]
groups=32
filters=32
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv2_1_linear
[convolutional]
filters=16
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# conv2_2_expand
[convolutional]
filters=96
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv2_2_dwise
[convolutional]
groups=96
filters=96
size=3
pad=1
stride=2
batch_normalize=1
activation=relu

# conv2_2_linear
[convolutional]
filters=24
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# conv3_1_expand
[convolutional]
filters=144
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv3_1_dwise
[convolutional]
groups=144
filters=144
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv3_1_linear
[convolutional]
filters=24
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_3_1
[shortcut]
from=-4
activation=linear

# conv_3_2_expand
[convolutional]
filters=144
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_3_2_dwise
[convolutional]
groups=144
filters=144
size=3
pad=1
stride=2
batch_normalize=1
activation=relu

# conv_3_2_linear
[convolutional]
filters=32
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# conv_4_1_expand
[convolutional]
filters=192
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_4_1_dwise
[convolutional]
groups=192
filters=192
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_1_linear
[convolutional]
filters=32
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_4_1
[shortcut]
from=-4
activation=linear

# conv_4_2_expand
[convolutional]
filters=192
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_4_2_dwise
[convolutional]
groups=192
filters=192
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_2_linear
[convolutional]
filters=32
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_4_2
[shortcut]
from=-4
activation=linear

# conv_4_3_expand
[convolutional]
filters=192
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_4_3_dwise
[convolutional]
groups=192
filters=192
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_3_linear
[convolutional]
filters=64
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# conv_4_4_expand
[convolutional]
filters=384
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_4_4_dwise
[convolutional]
groups=384
filters=384
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_4_linear
[convolutional]
filters=64
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_4_4
[shortcut]
from=-4
activation=linear

# conv_4_5_expand
[convolutional]
filters=384
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_4_5_dwise
[convolutional]
groups=384
filters=384
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_5_linear
[convolutional]
filters=64
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_4_5
[shortcut]
from=-4
activation=linear

# conv_4_6_expand
[convolutional]
filters=384
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_4_6_dwise
[convolutional]
groups=384
filters=384
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_6_linear
[convolutional]
filters=64
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_4_6
[shortcut]
from=-4
activation=linear

# conv_4_7_expand
[convolutional]
filters=384
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_4_7_dwise
[convolutional]
groups=384
filters=384
size=3
pad=1
stride=2
batch_normalize=1
activation=relu

# conv_4_7_linear
[convolutional]
filters=96
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# conv_5_1_expand
[convolutional]
filters=576
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_5_1_dwise
[convolutional]
groups=576
filters=576
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_5_1_linear
[convolutional]
filters=96
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_5_1
[shortcut]
from=-4
activation=linear

# conv_5_2_expand
[convolutional]
filters=576
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_5_2_dwise
[convolutional]
groups=576
filters=576
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_5_2_linear
[convolutional]
filters=96
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_5_2
[shortcut]
from=-4
activation=linear

# conv_5_3_expand
[convolutional]
filters=576
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_5_3_dwise
[convolutional]
groups=576
filters=576
size=3
pad=1
stride=2
batch_normalize=1
activation=relu

# conv_5_3_linear
[convolutional]
filters=160
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# conv_6_1_expand
[convolutional]
filters=960
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_6_1_dwise
[convolutional]
groups=960
filters=960
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_6_1_linear
[convolutional]
filters=160
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_6_1
[shortcut]
from=-4
activation=linear

# conv_6_2_expand
[convolutional]
filters=960
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_6_2_dwise
[convolutional]
groups=960
filters=960
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_6_2_linear
[convolutional]
filters=160
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_6_2
[shortcut]
from=-4
activation=linear

# conv_6_3_expand
[convolutional]
filters=960
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_6_3_dwise
[convolutional]
groups=960
filters=960
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_6_3_linear
[convolutional]
filters=320
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# conv_6_4
[convolutional]
filters=1280
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

######################

[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=1024
activation=leaky

[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=1024
activation=leaky

[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=1024
activation=leaky

[convolutional]
size=1
stride=1
pad=1
filters=75
activation=linear

[yolo]
mask = 6,7,8
anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
classes=20
num=9
jitter=.3
ignore_thresh = .5
truth_thresh = 1
random=1

[route]
layers = -4

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[upsample]
stride=2

[route]
layers = -1, 48



[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=512
activation=leaky

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=512
activation=leaky

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=512
activation=leaky

[convolutional]
size=1
stride=1
pad=1
filters=75
activation=linear

[yolo]
mask = 3,4,5
anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
classes=20
num=9
jitter=.3
ignore_thresh = .5
truth_thresh = 1
random=1

[route]
layers = -4

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[upsample]
stride=2

[route]
layers = -1, 37



[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky

[convolutional]
size=1
stride=1
pad=1
filters=75
activation=linear

[yolo]
mask = 0,1,2
anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
classes=20
num=9
jitter=.3
ignore_thresh = .5
truth_thresh = 1
random=1

